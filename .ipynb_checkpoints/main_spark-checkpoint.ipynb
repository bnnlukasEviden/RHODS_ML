{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dXdiP3wjZjD"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rxxKcQi3jZjE"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorAssembler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import sum, when, avg, max, col, dayofweek, count, month, lit, mean\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-s8lOJCnpH9",
    "outputId": "97cb486e-ffc2-4012-c226-6fc11b2ebed3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZK781OCjZjF"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STK-tQCrjZjF"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('RHODS').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jM4p9ZkjZjF"
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIZlgoT_jZjG"
   },
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkgSAi_HjZjG"
   },
   "outputs": [],
   "source": [
    "main_df = spark.read.csv('train.csv', header = True, inferSchema = True)\n",
    "oil_df = spark.read.csv('oil.csv', header = True, inferSchema = True)\n",
    "holidays_df = spark.read.csv('holidays_events.csv', header = True, inferSchema = True)\n",
    "transactions_df = spark.read.csv('transactions.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0ImIAC7jZjG"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEzaxAu9jZjG",
    "outputId": "fccda50f-a303-4848-cad6-0131a62118a6"
   },
   "outputs": [],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAF89StljZjG"
   },
   "source": [
    "## Train_df Aggregation, Date transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUH4N_s8jZjG",
    "outputId": "344050e9-2e17-4f39-8abd-39a042a743c9"
   },
   "outputs": [],
   "source": [
    "# Aggregation of the main_df to get the sales volumes aggregatet over all stores for each product family and date\n",
    "agg_df = main_df.groupBy('date', 'family').agg(sum('sales').alias(\"sales\"), sum('onpromotion').alias(\"onpromotion\"))\n",
    "\n",
    "# Adding of features related to the day of the week and the month of each record based on the date\n",
    "agg_df = agg_df.withColumn('day_of_week', dayofweek(agg_df.date)).withColumn('month', month(agg_df.date))\n",
    "agg_df.sort(col(\"date\"), col(\"family\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAmAeTufjZjG"
   },
   "source": [
    "## Oil_df Merge\n",
    "In this section the agg_df and the oil_df are getting merged to use the oil price in the ML-Part of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PTK1v3HjD5l"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v26AmAKkhjeR",
    "outputId": "a5cc092b-2ee9-4e38-9557-1912b8308f6e"
   },
   "outputs": [],
   "source": [
    "# Aggregating the oil_df and the agg_df\n",
    "agg_oil_merge_df = agg_df.join(oil_df, on='date', how='left') # left join agg_df and oil_df\n",
    "agg_oil_merge_df = agg_oil_merge_df.dropDuplicates(['date']) # reduce df to one row per date\n",
    "\n",
    "# Calculate the percentage of NaN values in the 'dcoilwtico' column\n",
    "percentage_nan = (agg_oil_merge_df.filter(col(\"dcoilwtico\").isNull()).count() / agg_oil_merge_df.count()) * 100\n",
    "print(f'Procentual number of NaN-Values: {percentage_nan}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgpR_JXKl3bX"
   },
   "source": [
    "Because of the high number of NaN-Values regarding the oilprice-value we decided to do a oilprice forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Nw4OtxmIin"
   },
   "source": [
    "### Oilprice forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnmaajceh1Mh"
   },
   "outputs": [],
   "source": [
    "# Create a subset of agg_oil_merge_df to forecast the oil prices\n",
    "oil_forecast_df = agg_oil_merge_df.select('date', 'dcoilwtico', 'day_of_week', 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hNbSyIZiFY7",
    "outputId": "cc8547a1-ef55-4d60-900f-42eef4723ee9"
   },
   "outputs": [],
   "source": [
    "n_lags = 5\n",
    "window_spec = Window.orderBy('date')\n",
    "for i in range(1, n_lags+1):\n",
    "  oil_forecast_df = oil_forecast_df.withColumn(\"lag_{}\".format(i), F.lag(\"dcoilwtico\", offset = i).over(window_spec).cast('float'))\n",
    "\n",
    "oil_forecast_df = oil_forecast_df.withColumn(\"dcoilwtico\", oil_forecast_df[\"dcoilwtico\"].cast('float'))\n",
    "oil_forecast_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWWLlJYejG-E"
   },
   "source": [
    "#### Data transformation/encoding/split, Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anZXDMBbjKF7"
   },
   "outputs": [],
   "source": [
    "numerical_cols = ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']\n",
    "categorical_cols = ['day_of_week', 'month']\n",
    "\n",
    "# Defining the encoder for encoding the categorical values\n",
    "encoder = OneHotEncoder(inputCols=categorical_cols,\n",
    "                        outputCols=[f\"{col}_encoded\" for col in categorical_cols])\n",
    "\n",
    "oil_forecast_encoded_df = encoder.fit(oil_forecast_df).transform(oil_forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqBnZW0XjizT"
   },
   "outputs": [],
   "source": [
    "# Create the train set of data with oil prices\n",
    "oil_fc_train = oil_forecast_encoded_df.filter(oil_forecast_encoded_df['dcoilwtico'].isNotNull())\n",
    "# Create the prediction set of data without oil prices\n",
    "oil_fc_pred = oil_forecast_encoded_df.filter(oil_forecast_encoded_df['dcoilwtico'].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtrUts9wkJcC"
   },
   "outputs": [],
   "source": [
    "# Set the imputer\n",
    "numerical_imputer = Imputer(strategy=\"median\", inputCols=numerical_cols, outputCols=[f\"{col}_imputed\" for col in numerical_cols])\n",
    "\n",
    "# Assemble features into a single vector\n",
    "feature_cols = [f\"{col}_imputed\" for col in numerical_cols] + [f\"{col}_encoded\" for col in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols,outputCol=\"features\")\n",
    "\n",
    "# Initialise the oilprice prediction model\n",
    "rfr_oil = RandomForestRegressor(featuresCol='features', labelCol='dcoilwtico')\n",
    "\n",
    "# Create the data preprocessing pipeline\n",
    "oil_pipeline = Pipeline(stages=[numerical_imputer, assembler, rfr_oil]) # pipeline for cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeFhqhSJlHvu"
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i71OPi1AlJ3d"
   },
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rfr_oil.maxDepth, [5,7]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"dcoilwtico\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "crossval = CrossValidator(estimator=oil_pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cv_model_oil = crossval.fit(oil_fc_train)\n",
    "\n",
    "best_model_oil = cv_model_oil.bestModel # Model with the best parameters\n",
    "\n",
    "oil_predictions = best_model_oil.transform(oil_fc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTu7DpZKlpat"
   },
   "source": [
    "#### Oil price dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6u7kCW5lnVm",
    "outputId": "0ea3a76d-ccf8-4b1b-b907-479e5dfc815e"
   },
   "outputs": [],
   "source": [
    "oil_new_df = oil_fc_train.select('date', 'dcoilwtico').union(oil_predictions.select('date', 'prediction').withColumnRenamed(\"prediction\",'dcoilwtico'))\n",
    "oil_new_df.sort(col(\"date\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwrT3XLLrm_X",
    "outputId": "71c80eea-1b54-4754-b367-887ca24e9ba6"
   },
   "outputs": [],
   "source": [
    "# Now the new_oil_df gets merged with the agg_df to do further preprocessing\n",
    "agg_oilprice_merged_df = agg_df.join(oil_new_df, on='date', how='left')\n",
    "\n",
    "# There are no NaN-Values left in the Column \"dcoilwtico\"\n",
    "percentage_nan = (agg_oilprice_merged_df.filter(col(\"dcoilwtico\").isNull()).count() / agg_oilprice_merged_df.count()) * 100\n",
    "print(f'Procentual number of NaN-Values: {percentage_nan}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6IV71ilmi7e"
   },
   "source": [
    "## Transactions_df Merge\n",
    "In this section we merge the aggregations_df with the agg_df to use the number of transaction per day for furter predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKraYTyUjZjI",
    "outputId": "8880365d-e4fe-45d4-c00c-6398a9571bc7"
   },
   "outputs": [],
   "source": [
    "agg_transactions_df = transactions_df.groupBy(\"date\").agg(sum(\"transactions\").alias(\"transactions\"))\n",
    "agg_transactions_df.sort(col(\"date\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3axBehCjZjI",
    "outputId": "d53cf22c-bcfb-427a-e704-2da1908133c3"
   },
   "outputs": [],
   "source": [
    "merged_df = agg_oilprice_merged_df.join(agg_transactions_df, on='date', how='left')\n",
    "merged_df = merged_df.na.drop()\n",
    "merged_df.sort(col(\"date\"), col(\"family\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzAIoTwgjZjI"
   },
   "source": [
    "### Holidays_df Merge\n",
    "In this section the holidays_df gets merged to use the transaction data for the upcoming model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmI1WELyjZjI",
    "outputId": "497e0a90-c2c2-4689-b8e9-ef3984cd7953"
   },
   "outputs": [],
   "source": [
    "holidays_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G2VEZrjjZjI",
    "outputId": "f92bfb8f-d22b-4c6d-b570-c906043b571a"
   },
   "outputs": [],
   "source": [
    "# We only want to use the Holiday/Additional,etc. data which counts for the whole country Ecuador\n",
    "modified_holidays_df = holidays_df.filter((col(\"transferred\") == False) & (col(\"locale\") == \"National\"))\n",
    "modified_holidays_df = modified_holidays_df.withColumn(\"type\", when(col(\"type\") == \"Transfer\", \"Holiday\").otherwise(col(\"type\")))\n",
    "\n",
    "modified_holidays_df = modified_holidays_df.select(\"date\", \"type\")\n",
    "modified_holidays_df.sort(col(\"date\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bv0_Yan0jZjI",
    "outputId": "95979385-4f0e-40c6-ff65-4020237d0524"
   },
   "outputs": [],
   "source": [
    "# Doing the merge of modified_holidays_df and merged_df\n",
    "merged_df = merged_df.join(modified_holidays_df, on=\"date\", how=\"left\")\n",
    "# The dates on which no holiday or something else encounted in the holiday_df takes place we insert \"Normal\" as a value\n",
    "merged_df = merged_df.withColumn(\"type\", when(merged_df[\"type\"].isNull(), \"Normal\").otherwise(merged_df[\"type\"]))\n",
    "merged_df.sort(col(\"date\"), col('family')).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kafBXltjZjI"
   },
   "source": [
    "# Feature based prediction (fb)\n",
    "In the first trained model for predicting the sales volumes we use a normal feature based prediction without taking into account previous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2JpI4uaXACO"
   },
   "source": [
    "## Data transformation/encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIWnx15yjZjI",
    "outputId": "2f3deee0-c7e6-4d5f-a2ab-817c0661fe72"
   },
   "outputs": [],
   "source": [
    "transformed_df_fb = merged_df\n",
    "\n",
    "transformed_df_fb = transformed_df_fb.withColumnRenamed(\"day_of_week\", \"day_of_week_index\")\n",
    "transformed_df_fb = transformed_df_fb.withColumnRenamed(\"month\", \"month_index\")\n",
    "\n",
    "str_cat_cols_fb = [\"type\", \"family\"]\n",
    "cat_cols_fb = [\"day_of_week\", \"month\", \"type\", \"family\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in str_cat_cols_fb]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in cat_cols_fb]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "model = pipeline.fit(transformed_df_fb)\n",
    "data = model.transform(transformed_df_fb)\n",
    "data.sort(col(\"date\"), col('family')).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePBuO71rXBUQ"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EkUS3gCjZjI"
   },
   "outputs": [],
   "source": [
    "feature_cols = [\"day_of_week_encoded\", \"month_encoded\", \"type_encoded\", \"family_encoded\", \"transactions\", \"dcoilwtico\", \"onpromotion\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"sales\", maxBins=33)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [4, 6])\n",
    "             .addGrid(gbt.maxIter, [50, 100])\n",
    "             .addGrid(gbt.stepSize, [0.1, 0.01])\n",
    "             .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=12)\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9y3A2VN5MZr"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiD2R0gW5P1b"
   },
   "source": [
    "### Overall evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlYC9hxk5L67",
    "outputId": "977b167e-5a92-4e7c-e914-d468557a49f0"
   },
   "outputs": [],
   "source": [
    "evaluation_fb_df = cvModel.transform(test_data)\n",
    "clipped_evaluation_fb_df = evaluation_fb_df.withColumn(\"clipped_predictions\", when(col(\"prediction\") < 0, 0).otherwise(col(\"prediction\")))\n",
    "\n",
    "mae = evaluator.evaluate(clipped_evaluation_fb_df)\n",
    "\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7ac8vuEjZjc",
    "outputId": "0ee4158a-2c27-4d36-a295-3f6a7f559086"
   },
   "outputs": [],
   "source": [
    "# Show the true sales values compared to the predicted ones\n",
    "clipped_evaluation_fb_df.select(\"sales\", \"clipped_predictions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG3tl8ObjZjc"
   },
   "source": [
    "### Product family wise evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAJjUtKe8mK-"
   },
   "outputs": [],
   "source": [
    "filtered_evaluation_fb_df = clipped_evaluation_fb_df.select(\"family\", \"sales\", \"clipped_predictions\")\n",
    "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"clipped_predictions\", metricName=\"mae\")\n",
    "unique_family_values = filtered_evaluation_fb_df.select(\"family\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "data = []\n",
    "for value in unique_family_values:\n",
    "  df_fb = filtered_evaluation_fb_df.filter(filtered_evaluation_fb_df['family'] == value)\n",
    "  mae_fb = evaluator.evaluate(df_fb)\n",
    "  mean_fb = df_fb.select(mean(\"sales\")).collect()[0][0]\n",
    "  row = Row(family=value, mean=mean_fb, mean_absolute_error=mae_fb)\n",
    "  data.append(row)\n",
    "\n",
    "family_evaluation_fb_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ghgw2zy_FPy3",
    "outputId": "8a1f4d74-6ebf-44c9-e6e9-bb32ce447636"
   },
   "outputs": [],
   "source": [
    "family_evaluation_fb_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAR7KybXjZjd"
   },
   "source": [
    "# Feature based prediction with Time lags (tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS2V5KaJWvw3"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_9PYDECV43e"
   },
   "outputs": [],
   "source": [
    "tl_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2UuyDzjPLkb"
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"family\").orderBy(\"date\")\n",
    "\n",
    "tl_df = tl_df.withColumn(\"lag_1\", F.lag(\"sales\", 1).over(window_spec))\n",
    "tl_df = tl_df.withColumn(\"lag_2\", F.lag(\"sales\", 2).over(window_spec))\n",
    "tl_df = tl_df.withColumn(\"lag_3\", F.lag(\"sales\", 3).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RchUStfCV27I",
    "outputId": "e048afa2-6f97-4fcb-eabd-960eb0347fac"
   },
   "outputs": [],
   "source": [
    "specific_date = \"2013-01-04\" # The Time lag dataframe should start from the \"2013-01-04\" because else there is no lag data for the first row\n",
    "specific_date = spark.createDataFrame([(specific_date,)], [\"specific_date\"]).withColumn(\"specific_date\", col(\"specific_date\").cast(DateType()))\n",
    "tl_df_filtered = tl_df.filter(col(\"date\") >= specific_date.select(\"specific_date\").collect()[0][0])\n",
    "tl_df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GpTYtT4W0Qf"
   },
   "source": [
    "## Data transformation/encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtV4qfH0WyIf",
    "outputId": "3cd48a94-c98c-4cac-c805-397940634c44"
   },
   "outputs": [],
   "source": [
    "transformed_df_tl = tl_df_filtered\n",
    "\n",
    "transformed_df_tl = transformed_df_tl.withColumnRenamed(\"day_of_week\", \"day_of_week_index\")\n",
    "transformed_df_tl = transformed_df_tl.withColumnRenamed(\"month\", \"month_index\")\n",
    "\n",
    "str_cat_cols_tl = [\"type\", \"family\"]\n",
    "cat_cols_tl = [\"day_of_week\", \"month\", \"type\", \"family\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in str_cat_cols_tl]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in cat_cols_tl]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "model = pipeline.fit(transformed_df_tl)\n",
    "data = model.transform(transformed_df_tl)\n",
    "data.sort(col(\"date\"), col('family')).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwMVNNTBXG1w"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TluQKFiuXMCb"
   },
   "outputs": [],
   "source": [
    "feature_cols = [\"day_of_week_encoded\", \"month_encoded\", \"type_encoded\", \"family_encoded\", \"transactions\", \"dcoilwtico\", \"onpromotion\", \"lag_1\", \"lag_2\", \"lag_3\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"sales\", maxBins=33)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [4, 6])\n",
    "             .addGrid(gbt.maxIter, [50, 100])\n",
    "             .addGrid(gbt.stepSize, [0.1, 0.01])\n",
    "             .build())\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=12)\n",
    "cvModel = crossval.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71KROdBtXNfm"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DXdivWnXS5v"
   },
   "source": [
    "### Overall evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVqKeGn7XU5v",
    "outputId": "5d81a14d-f6e8-487b-e005-edc3da0b0e09"
   },
   "outputs": [],
   "source": [
    "evaluation_tl_df = cvModel.transform(test_data)\n",
    "clipped_evaluation_tl_df = evaluation_tl_df.withColumn(\"clipped_predictions\", when(col(\"prediction\") < 0, 0).otherwise(col(\"prediction\")))\n",
    "\n",
    "mae_tl = evaluator.evaluate(evaluation_tl_df)\n",
    "\n",
    "best_model_tl = cvModel.bestModel\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5G0PziAbXYnw",
    "outputId": "1978cfb4-aaa3-4b5a-88ae-ed133d85577d"
   },
   "outputs": [],
   "source": [
    "# Show the true sales values compared to the predicted ones\n",
    "clipped_evaluation_tl_df.select(\"sales\", \"clipped_predictions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qx1YhD-XcpA"
   },
   "source": [
    "### Product family wise evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Y79yIoXXgZm"
   },
   "outputs": [],
   "source": [
    "filtered_evaluation_fb_df = clipped_evaluation_tl_df.select(\"family\", \"sales\", \"clipped_predictions\")\n",
    "evaluator = RegressionEvaluator(labelCol=\"sales\", predictionCol=\"clipped_predictions\", metricName=\"mae\")\n",
    "unique_family_values = filtered_evaluation_fb_df.select(\"family\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "data = []\n",
    "for value in unique_family_values:\n",
    "  df_fb = filtered_evaluation_fb_df.filter(filtered_evaluation_fb_df['family'] == value)\n",
    "  mae_fb = evaluator.evaluate(df_fb)\n",
    "  mean_fb = df_fb.select(mean(\"sales\")).collect()[0][0]\n",
    "  row = Row(family=value, mean=mean_fb, mean_absolute_error=mae_fb)\n",
    "  data.append(row)\n",
    "\n",
    "family_evaluation_fb_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRUikUc7Xkge",
    "outputId": "87331eaf-7164-400e-946f-f9819628f822"
   },
   "outputs": [],
   "source": [
    "family_evaluation_fb_df.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
